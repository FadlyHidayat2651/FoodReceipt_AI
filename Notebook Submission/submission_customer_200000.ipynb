{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394c4c59",
   "metadata": {},
   "source": [
    "# Low Memory Data Parsing\n",
    "\n",
    "In this task I will try to parse data by making sure that it will keep the memory low\n",
    "\n",
    "**Definition**\n",
    "Low memory usage as the ability of a data processing framework to keep its peak resident memory footprint minimal, predictable, and well below a reasonable fraction of available system RAM, while maintaining correct execution and performance for large datasets.\n",
    "\n",
    "Source:\n",
    "1. https://www.ece.ucdavis.edu/~hhomayou/files/hosein-ccgrid.pdf\n",
    "2. https://dl.acm.org/doi/10.1109/CCGRID.2018.00097\n",
    "3. https://gac.udc.es/~juan/papers/encyclopedia2022.pdf\n",
    "\n",
    "In order to make sure that our process can have low memory parsing, we will try to do comparison between four methods on running or parsing data in python.\n",
    "1. Standard Pandas Data Processing: Pandas is the most widely-used data processing library in Python, making it the natural baseline for memory comparisons. It loads data fully into memory, so it clearly shows traditional bottlenecks in low-memory environments.\n",
    "2. Pyspark data processing: PySpark supports distributed computing, making it suitable for scaling large datasets beyond local RAM. Comparing PySpark helps us understand how much overhead a JVM-based execution engine adds compared to pure-Python solutions.\n",
    "3. Polar data processing: Polars uses Apache Arrow and Rust for fully optimized columnar processing, enabling significantly lower memory usage than Pandas. Including Polars highlights the benefits of modern, memory-efficient DataFrame technologies.\n",
    "4. DuckDB (Lazy Mode): DuckDB processes data lazily using vectorized execution, reducing peak memory by avoiding full in-memory loading. This mode shows how far memory usage can be minimized when computation is pushed to the storage layer.\n",
    "\n",
    "There are five parameters that we can use in order to make sure that we can see the low memory usage.\n",
    "1. **Peak Memory Usage** : Maximum resident memory used during executions\n",
    "2. **Memory Efficiency per Data Volume**: Memory used per million rows/ GB processed\n",
    "3. **Memory Stability**: Fluctuations in memory overtime\n",
    "4. **Garbage or Cache Reclaim**: Ability to free memory after expensive operations\n",
    "5. **Scalability**: Behavior as data increases (50K - 100K - 200K) row of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaea623",
   "metadata": {},
   "source": [
    "## Help Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa11e87",
   "metadata": {},
   "source": [
    "Make sure you can install the pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1b3eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10a3b7310>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/muhammadfadlyhidayat/miniconda3/envs/deepsearch_v2/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/23 11:00:31 WARN Utils: Your hostname, Muhammads-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.18.24 instead (on interface en0)\n",
      "25/11/23 11:00:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/23 11:00:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.range(5).show()\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d62d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Initialize Spark\n",
    "findspark.init()\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MemoryBenchmark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "### ðŸ“Œ Memory helpers ###\n",
    "def memory_used_mb():\n",
    "    return psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# JVM memory for Spark\n",
    "def spark_memory_mb():\n",
    "    jvm = spark._sc._jvm\n",
    "    runtime = jvm.java.lang.Runtime.getRuntime()\n",
    "    return (runtime.totalMemory() - runtime.freeMemory()) / (1024 * 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f945fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import duckdb\n",
    "\n",
    "def load_pandas(f):\n",
    "    return pd.read_csv(f)\n",
    "\n",
    "def load_polars(f):\n",
    "    return pl.read_csv(f)\n",
    "\n",
    "def load_spark(f):\n",
    "    df = spark.read.csv(f, header=True, inferSchema=True).cache()\n",
    "    df.count()  # force caching\n",
    "    return df\n",
    "\n",
    "# DuckDB: Lazy (does not load fully into memory)\n",
    "def load_duckdb_lazy(f):\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "    df = con.execute(f\"SELECT * FROM read_csv_auto('{f}')\").pl()  # Lazily evaluated\n",
    "    con.close()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858d44f",
   "metadata": {},
   "source": [
    "## Metrics Evaluations Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def94d83",
   "metadata": {},
   "source": [
    "### Peak Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13499ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_peak_memory(name, load_fn, file):\n",
    "    if name == \"PySpark\":\n",
    "        before = spark_memory_mb()\n",
    "        df = load_fn(file)\n",
    "        after = spark_memory_mb()\n",
    "    else:\n",
    "        before = memory_used_mb()\n",
    "        df = load_fn(file)\n",
    "        after = memory_used_mb()\n",
    "\n",
    "    return df, round(after - before, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de563dfa",
   "metadata": {},
   "source": [
    "### Memory Efficiency (MB/Million rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92a66b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_efficiency(peak, rows):\n",
    "    return round(peak / (rows / 1_000_000), 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97a2b0",
   "metadata": {},
   "source": [
    "### Memory Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22629824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_stability(df, name):\n",
    "    mem = []\n",
    "    for _ in range(3):\n",
    "        if name == \"PySpark\":\n",
    "            df.limit(1000).count()\n",
    "            mem.append(spark_memory_mb())\n",
    "        else:\n",
    "            df.head(1000)\n",
    "            mem.append(memory_used_mb())\n",
    "    return round(pd.Series(mem).std(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ecab5",
   "metadata": {},
   "source": [
    "### Garbage Reclaim Ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d0a0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def garbage_reclaim(df, name):\n",
    "    if name == \"PySpark\":\n",
    "        df.unpersist()\n",
    "        before = spark_memory_mb()\n",
    "        spark.catalog.clearCache()\n",
    "        gc.collect()\n",
    "        after = spark_memory_mb()\n",
    "    else:\n",
    "        before = memory_used_mb()\n",
    "        del df\n",
    "        gc.collect()\n",
    "        after = memory_used_mb()\n",
    "\n",
    "    return round(before - after, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41185ba4",
   "metadata": {},
   "source": [
    "### Scalability Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "465aff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalability(loader, name, file):\n",
    "    sizes = [50_000, 100_000, 200_000]\n",
    "    mem = []\n",
    "\n",
    "    for size in sizes:\n",
    "        df = loader(file)\n",
    "        if name == \"PySpark\":\n",
    "            df.limit(size).count()\n",
    "            mem.append(spark_memory_mb())\n",
    "            df.unpersist()\n",
    "        else:\n",
    "            df = df.head(size)\n",
    "            mem.append(memory_used_mb())\n",
    "        gc.collect()\n",
    "\n",
    "    return round((mem[-1] - mem[0]) / (sizes[-1] - sizes[0]) * 1000, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31dd997",
   "metadata": {},
   "source": [
    "## Full Benchmark Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96451211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:14 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:22 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:23 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/11/23 11:01:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Framework</th>\n",
       "      <th>Peak Memory (MB)</th>\n",
       "      <th>Memory / 1M rows (MB)</th>\n",
       "      <th>Memory Stability (std)</th>\n",
       "      <th>Memory Reclaimed (MB)</th>\n",
       "      <th>Scalability (MB / 1K rows growth)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pandas</td>\n",
       "      <td>1412.22</td>\n",
       "      <td>706.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polars</td>\n",
       "      <td>642.67</td>\n",
       "      <td>321.335</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PySpark</td>\n",
       "      <td>567.55</td>\n",
       "      <td>283.775</td>\n",
       "      <td>112.017</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DuckDB Lazy</td>\n",
       "      <td>161.06</td>\n",
       "      <td>80.530</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Framework  Peak Memory (MB)  Memory / 1M rows (MB)  \\\n",
       "0       Pandas           1412.22                706.110   \n",
       "1       Polars            642.67                321.335   \n",
       "2      PySpark            567.55                283.775   \n",
       "3  DuckDB Lazy            161.06                 80.530   \n",
       "\n",
       "   Memory Stability (std)  Memory Reclaimed (MB)  \\\n",
       "0                   0.000                   0.00   \n",
       "1                   0.000                   4.00   \n",
       "2                 112.017                  -0.54   \n",
       "3                   0.000                   0.00   \n",
       "\n",
       "   Scalability (MB / 1K rows growth)  \n",
       "0                              3.469  \n",
       "1                              0.891  \n",
       "2                              0.758  \n",
       "3                             -1.628  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE = \"xx/customers-2000000.csv\"\n",
    "\n",
    "frameworks = [\n",
    "    (\"Pandas\", load_pandas),\n",
    "    (\"Polars\", load_polars),\n",
    "    (\"PySpark\", load_spark),\n",
    "    (\"DuckDB Lazy\", load_duckdb_lazy)\n",
    "]\n",
    "\n",
    "\n",
    "results = []\n",
    "for name, loader in frameworks:\n",
    "    record = {\"Framework\": name}\n",
    "\n",
    "    df, peak = measure_peak_memory(name, loader, FILE)\n",
    "    record[\"Peak Memory (MB)\"] = peak\n",
    "\n",
    "    if name == \"PySpark\":\n",
    "        rows = df.count()\n",
    "    elif name == \"DuckDB-Lazy\":\n",
    "        rows = df.count()[0][0]\n",
    "    else:\n",
    "        rows = df.shape[0]\n",
    "\n",
    "    record[\"Memory / 1M rows (MB)\"] = memory_efficiency(peak, rows)\n",
    "    record[\"Memory Stability (std)\"] = memory_stability(df, name)\n",
    "    record[\"Memory Reclaimed (MB)\"] = garbage_reclaim(df, name)\n",
    "    record[\"Scalability (MB / 1K rows growth)\"] = scalability(loader, name, FILE)\n",
    "\n",
    "    results.append(record)\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c54f2",
   "metadata": {},
   "source": [
    "### Summary\n",
    "1. Peak Memory (MB)\n",
    "Highest: Pandas â€” consumes the most memory when loading data\n",
    "Lowest: DuckDB Lazy â€” very efficient, minimal loading into RAM\n",
    "Polars & PySpark fall in the middle, far better than Pandas\n",
    "\n",
    "2. Memory / 1M Rows (MB)\n",
    "Pandas: high memory-per-row â†’ not scalable\n",
    "Polars & PySpark: much lower footprint â†’ strong scalability\n",
    "DuckDB Lazy: best efficiency â†’ ideal for large datasets\n",
    "\n",
    "3. Memory Stability (std)\n",
    "Polars, Pandas, DuckDB Lazy: extremely stable memory usage\n",
    "PySpark: variable memory usage due to dynamic JVM/executor behavior\n",
    "\n",
    "4. Memory Reclaimed (MB)\n",
    "PySpark & Polars reclaim memory after execution\n",
    "Pandas & DuckDB Lazy do not show active memory release during this test\n",
    "\n",
    "5. Scalability (Memory Growth Rate)\n",
    "DuckDB Lazy: negative growth â†’ exceptional memory reuse\n",
    "PySpark & Polars: small incremental growth â†’ scalable\n",
    "Pandas: sharp increase â†’ not suitable for growing data sizes\n",
    "\n",
    "### Conclusion\n",
    "For large-scale and low-memory workloads, DuckDB Lazy is the most efficient, Polars provides the best balance of speed and stability, PySpark is useful only when distributed computing is needed, while Pandas is the least scalable option"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsearch_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
